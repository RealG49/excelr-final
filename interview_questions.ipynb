{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ccf69a",
   "metadata": {},
   "source": [
    "\n",
    "# Interview Questions and Answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64bdaf",
   "metadata": {},
   "source": [
    "\n",
    "## 1. What is Normalization & Standardization and how is it helpful?\n",
    "\n",
    "**Normalization** and **Standardization** are two techniques used to rescale numerical features in a dataset:\n",
    "\n",
    "- **Normalization** (Min-Max Scaling) transforms data to a fixed range, typically [0,1] or [-1,1], using the formula:\n",
    "  \\[\n",
    "  X_{norm} = \f",
    "rac{X - X_{min}}{X_{max} - X_{min}}\n",
    "  \\]\n",
    "  This is useful when features have different scales and when algorithms (e.g., k-NN, neural networks) assume data in a bounded range.\n",
    "\n",
    "- **Standardization** (Z-score Scaling) rescales data to have a mean of 0 and a standard deviation of 1:\n",
    "  \\[\n",
    "  X_{std} = \f",
    "rac{X - \\mu}{\\sigma}\n",
    "  \\]\n",
    "  This is useful for algorithms that assume normally distributed data, like linear regression and PCA.\n",
    "\n",
    "**Benefits:**\n",
    "- Improves numerical stability of models\n",
    "- Ensures fair weightage for different features\n",
    "- Enhances performance of gradient-based optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Normalization and Standardization are two techniques used to rescale numerical features.\")\n",
    "print(\"\n",
    "Normalization (Min-Max Scaling) transforms data to a fixed range, typically [0,1] or [-1,1].\")\n",
    "print(\"Formula: X_norm = (X - X_min) / (X_max - X_min)\")\n",
    "print(\"\n",
    "Standardization (Z-score Scaling) rescales data to have a mean of 0 and a standard deviation of 1.\")\n",
    "print(\"Formula: X_std = (X - mean) / std_dev\")\n",
    "print(\"\n",
    "Benefits:\n",
    "- Improves numerical stability of models\n",
    "- Ensures fair weightage for different features\n",
    "- Enhances performance of gradient-based optimizers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21347367",
   "metadata": {},
   "source": [
    "\n",
    "## 2. What techniques can be used to address multicollinearity in multiple linear regression?\n",
    "\n",
    "Multicollinearity occurs when independent variables are highly correlated, which can distort coefficient estimates in multiple linear regression.\n",
    "\n",
    "**Techniques to address multicollinearity:**\n",
    "\n",
    "1. **Variance Inflation Factor (VIF):** Compute VIF for each predictor and remove variables with high VIF (>10).\n",
    "2. **Correlation Matrix:** Identify highly correlated pairs and drop or combine them.\n",
    "3. **Principal Component Analysis (PCA):** Transform correlated features into uncorrelated principal components.\n",
    "4. **Feature Selection:** Use stepwise regression, Lasso regression, or domain knowledge to select key predictors.\n",
    "5. **Remove Redundant Variables:** Drop one of the correlated variables manually if the information overlap is high.\n",
    "6. **Centering the Variables:** Mean-centering variables (subtracting the mean) can sometimes reduce collinearity effects.\n",
    "\n",
    "These techniques help in improving the stability and interpretability of the regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe1bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
